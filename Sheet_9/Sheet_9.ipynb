{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10758e62",
   "metadata": {},
   "source": [
    "# Interpretable Machine Learning\n",
    "## Exercise Sheet: 9\n",
    "## This exercise sheet covers chapters 10.1 and 10.2 from the IML book by Christoph Molnar\n",
    "Kristin Blesch (blesch@leibniz-bips.de)<br>\n",
    "Niklas Koenen (koenen@leibniz-bips.de)\n",
    "<hr style=\"border:1.5px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed6c18",
   "metadata": {},
   "source": [
    "# 1) Learned Features\n",
    "\n",
    "## a) Feature Visualization\n",
    "\n",
    "**I)** Describe the Feature Visualization method and formulate this method as an optimization problem for a single neuron, a channel, and the entire layer. Which approaches are available for solving this optimization problem?\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e9c816",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6abce3e1",
   "metadata": {},
   "source": [
    "**II)** Consider the following neural network with two layers for a classification problem with two classes and input $x \\in \\mathbb{R}^2$:\n",
    "$$\\text{model}(x) = \\text{softmax}\\Big( W_2 \\cdot \\text{ReLU}\\big(W_1 \\cdot x + b_1 \\big) + b_2 \\Big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998d73cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define model weights and bias\n",
    "W1 = torch.tensor(\n",
    "    (( 0.5006,  0.2231),\n",
    "     (-0.8746, -1.0410),\n",
    "     ( 0.1379,  0.0027),\n",
    "     ( 0.0556, -0.3049)))\n",
    "b1 = torch.tensor((0.5872, 0.9070, -0.6963, 0.2474))\n",
    "\n",
    "W2 = torch.tensor(\n",
    "    (( 0.9417, -0.5311, -0.1064, -0.4385),\n",
    "     (-0.5608,  0.3371, -0.0343, -0.2047)))\n",
    "b2 = torch.tensor((0.1320, 0.1329))\n",
    "\n",
    "# Define model\n",
    "def model(x):\n",
    "  # First layer\n",
    "  x = torch.matmul(x, W1.t()) + b1\n",
    "  x = torch.relu(x)\n",
    "\n",
    "  # Second layer\n",
    "  x = torch.matmul(x, W2.t()) + b2\n",
    "  x = torch.softmax(x, dim = 1)\n",
    "  \n",
    "  return x\n",
    "\n",
    "# Create a n x n grid and the coordinates for the contour plot\n",
    "n = 100\n",
    "x, y = np.meshgrid(np.linspace(-10,10,n),np.linspace(-10,10,n))\n",
    "coords = np.stack((x.reshape(-1), y.reshape(-1)), axis = 1)\n",
    "coords_torch = torch.tensor(coords, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a4b045",
   "metadata": {},
   "source": [
    "Instead of optimizing a unit for only one input, we now create a contour plot ([`matplotlib.pyplot.contourf`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contourf.html#matplotlib-pyplot-contourf)) to see the activation for all possible inputs from $\\mathbb{R}^2$ (this is only possible in this case because the input space is 2-dimensional).  \n",
    "\n",
    "- Create these plots once for the first neuron in the second layer (before Softmax) and for the second neuron in the first layer (after ReLU).\n",
    "- Which value solves (globally on the whole plane) the optimization problem in each case, or can this question be answered at all? If not, how is this problem tackled?\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f295781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the activation of neuron 1 in layer 2 (before softmax)\n",
    "def get_act_layer2_neuron1(x):\n",
    "  #\n",
    "  # to do!\n",
    "  #\n",
    "  return x\n",
    "\n",
    "# Create the contour plot\n",
    "\n",
    "# Get activations for selected unit and convert it from torch to numpy\n",
    "z = get_act_layer2_neuron1(coords_torch).numpy()\n",
    "# Reshape z to an n x n matrix\n",
    "z = z.reshape((n,n))\n",
    "\n",
    "# Plot the result\n",
    "plt.contourf(x, y, z, levels = 20, cmap = 'bwr', vmin = -np.max(np.abs(z)), vmax = np.max(np.abs(z)))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff720d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the activation of neuron 2 in layer 1 (after ReLU)\n",
    "def get_act_layer1_neuron2(x):\n",
    "  #\n",
    "  # to do!\n",
    "  #\n",
    "  return x\n",
    "\n",
    "# Create the contour plot\n",
    "\n",
    "# Get activations for selected unit and convert it from torch to numpy\n",
    "z = get_act_layer1_neuron2(coords_torch).numpy()\n",
    "# Reshape z to an n x n matrix\n",
    "z = z.reshape((n,n))\n",
    "\n",
    "# Plot the result\n",
    "plt.contourf(x, y, z, levels = 20, cmap = 'bwr', vmin = -np.max(np.abs(z)), vmax = np.max(np.abs(z)))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c18f6",
   "metadata": {},
   "source": [
    "## b) Network Dissection\n",
    "\n",
    "**I)** What is the difference between the Feature Visualization and Network Dissection methods?\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0511d922",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2356ae23",
   "metadata": {},
   "source": [
    "**II)** Explain the three steps of the network dissection algorithm.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70314711",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b26c8db3",
   "metadata": {},
   "source": [
    "# 2) Pixel Attribution\n",
    "\n",
    "## a) Theory\n",
    "\n",
    "**I)** What is the basic idea behind pixel attribution methods and what is the mathematical legitimization of using the gradients to interpret the model prediction?\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a617f9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b703796f",
   "metadata": {},
   "source": [
    "**II)** Explain the intuition behind the Grad-CAM method and describe the corresponding algorithm.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a1dfbd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60fabfdc",
   "metadata": {},
   "source": [
    "## b) Programming exercise\n",
    "In this task, we use the pretrained network [Inception v3](https://arxiv.org/abs/1512.00567) in `torch` and `torchvision` to apply some pixel attribution methods on the following image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b6cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                          std=[0.229, 0.224, 0.225]),\n",
    "     transforms.Resize((299,299))])\n",
    "\n",
    "# Define plot function for torch.tensors of form (1, C, H, W)\n",
    "def plot_image(x):\n",
    "  # take the sum of the absolut values over the channels\n",
    "  x = x[0,].abs().sum(dim = 0)\n",
    "  # convert it to numpy and transform it to make artifacts more explicit\n",
    "  x = x.numpy()**0.75\n",
    "  plt.imshow(x)\n",
    "  plt.show()\n",
    "\n",
    "# Load and preprocess image\n",
    "img = Image.open('imagenet_rooster.png') # Make sure that the image is in your working directory!!!\n",
    "x = transform(img)  # Preprocess image\n",
    "x = x.unsqueeze(0)\n",
    "\n",
    "# Get model from torchvision\n",
    "model = models.inception_v3(pretrained = True)\n",
    "model.eval()\n",
    "\n",
    "# Show image\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d62113",
   "metadata": {},
   "source": [
    "**I)** Calculate the prediction for the image for the Inception v3 model as a probability and output the index with the highest probability. Hint: The model outputs the pre-softmax values, i.e. the last layer has no activation function.  \n",
    "**Bonus:** What label does this index correspond to?\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47da9b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b642b368",
   "metadata": {},
   "source": [
    "**II)** Apply the Vanilla Gradient method to the image for the model output with the highest probability from the last subtask. Plot your result.  \n",
    "**Hint:** For a scalar output `out`, the method `out.backward()` calculates the gradients of the output with respect to the inputs `input`. Afterwards the gradients can be output with `input.grad`.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the torch-converted image\n",
    "input = x.clone()\n",
    "# Tell torch to track the gradients for this value\n",
    "input.requires_grad = True\n",
    "\n",
    "#\n",
    "# to do\n",
    "#\n",
    "# grad = ...\n",
    "# plot_image(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a9c3a4",
   "metadata": {},
   "source": [
    "**III)** Apply the Grad-CAM method to the image for the model output with the highest probability from task **I)** and plot your result.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e4867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to define a hook to calculate the gradients of the output w.r.t. the last convolutional layer.\n",
    "# Hooks are executed in addition to the forward pass and in this case only the input will be stored.\n",
    "hook = {}\n",
    "def get_input_hook(self, input, output):\n",
    "  hook['input'] = input[0]\n",
    "\n",
    "# We add the defined hook in the layer 'avgpool' which is the first non-colvolutional layer and its input \n",
    "# is of size (*, 2048, 8, 8)\n",
    "model.avgpool.register_forward_hook(get_input_hook)\n",
    "\n",
    "# Clone the torch-converted image and tell torch to track the gradients for this input\n",
    "input = x.clone()\n",
    "input.requires_grad = True\n",
    "\n",
    "# to do (start) -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# get the model output with the highest class probability\n",
    "# out = ...\n",
    "\n",
    "# Calculate gradients w.r.t to the last convolutional layer\n",
    "grad_A_k = torch.autograd.grad(out, hook['input'])[0]\n",
    "\n",
    "# Calculate a_k\n",
    "# a_k = ...\n",
    "\n",
    "# Get the feature maps A_k (stored in the forward-hook)\n",
    "# A_k = ...\n",
    "\n",
    "# Calculate the localization map (ReLU(sum(a_k * A_k)))\n",
    "# grad_cam = ...\n",
    "\n",
    "# to do (end) -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Plot the result\n",
    "plot_image(grad_cam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f10e0b",
   "metadata": {},
   "source": [
    "**IV)** Combine both previous results in the Guided Grad-CAM method and plot your result. Use the function [`torch.nn.Upsample`](https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html#torch.nn.Upsample) for upsampling the Grad-CAM result.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1639592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample the Grad-CAM result to size (1,1,299,299)\n",
    "# grad_cam_ups = ...\n",
    "\n",
    "# Multiply heatmaps\n",
    "# guided_grad_cam = ...\n",
    "\n",
    "# Plot the result\n",
    "plot_image(guided_grad_cam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
