{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10758e62",
   "metadata": {},
   "source": [
    "# Interpretable Machine Learning\n",
    "## Exercise Sheet: 7\n",
    "## This exercise sheet covers chapters 9.2 and 9.3 from the IML book by Christoph Molnar\n",
    "Kristin Blesch (blesch@leibniz-bips.de)<br>\n",
    "Niklas Koenen (koenen@leibniz-bips.de)\n",
    "<hr style=\"border:1.5px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd48f8",
   "metadata": {},
   "source": [
    "# 1) LIME \n",
    "\n",
    "**a)** Please describe the general idea behind LIME and how the procedure works. Make sure to explain why this method is considered local and what a surrogate model is.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8f5de0",
   "metadata": {},
   "source": [
    "Relevant passages of text from 9.2:\n",
    "\"LIME focuses on training local surrogate models to explain individual predictions.\"\n",
    "\"Local surrogate models are interpretable models that are used to explain individual predictions of black box machine learning models.\"\n",
    "\"LIME tests what happens to the predictions when you give variations of your data into the machine learning model. LIME generates a new dataset consisting of perturbed samples and the corresponding predictions of the black box model. On this new dataset LIME then trains an interpretable model, which is weighted by the proximity of the sampled instances to the instance of interest.\"\n",
    "\"The recipe for training local surrogate models:\n",
    "- Select your instance of interest for which you want to have an explanation of its black box prediction.\n",
    "- Perturb your dataset and get the black box predictions for these new points.\n",
    "- Weight the new samples according to their proximity to the instance of interest.\n",
    "- Train a weighted, interpretable model on the dataset with the variations.\n",
    "- Explain the prediction by interpreting the local model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae288f5",
   "metadata": {},
   "source": [
    "**b)** Lime with tabular data\n",
    "\n",
    "For an application of LIME, consider the gradient boosted california housing model you fitted last week (run code below). \n",
    "\n",
    "Use LIME to give local explanations of the first and the fifth instance of the test data! Use a local ridge regression (default), discretize the data and then give an explanation using 4 features only. Visualize your results using an adequate plot and interpret the output! \n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbfbe8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76154181, 1.62701009, 2.59560924, ..., 2.73226183, 4.32333119,\n",
       "       0.59749722])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get data\n",
    "cal_housing = fetch_california_housing()\n",
    "X = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\n",
    "y = cal_housing.target\n",
    "\n",
    "# Get train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Train model and calculate R²-score\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "model = HistGradientBoostingRegressor(random_state = 42)\n",
    "model.fit(X_train, y_train)\n",
    "model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explaination using LIME\n",
    "import numpy as np\n",
    "import lime.lime_tabular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7a8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(np.array(X_train),\n",
    "                    feature_names=cal_housing.feature_names, \n",
    "                    discretize_continuous = True,              \n",
    "                    verbose=True, mode='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6fb2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(X_test.iloc[0], \n",
    "     model.predict,\n",
    "     num_features=4) # using 4 features only\n",
    "expe = explainer.explain_instance(X_test.iloc[4], \n",
    "     model.predict, num_features=4) # using 4 features only\n",
    "expe.as_pyplot_figure()\n",
    "exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed6c18",
   "metadata": {},
   "source": [
    "# 2) Counterfactual Explanations\n",
    "\n",
    "\n",
    "What is a counterfactual explanation? Explain the concept using an example. Furthermorre, describe how this perspective applies to the context of interpretable machine learning: Do we explain the causal structure of the data?What makes a counterfactual explanation a good explanation? \n",
    "\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e328721",
   "metadata": {},
   "source": [
    "Relevant passages of text from 9.3: \n",
    "\"A counterfactual explanation describes a causal situation in the form: “If X had not occurred, Y would not have occurred”.\" \n",
    "\"Thinking in counterfactuals requires imagining a hypothetical reality that contradicts the observed facts (for example, a world in which I have not drunk the hot coffee), hence the name “counterfactual”.\"\n",
    "\"Even if in reality the relationship between the inputs and the outcome to be predicted might not be causal, we can see the inputs of a model as the cause of the prediction. The inputs cause the prediction (not necessarily reflecting the real causal relation of the data).\"\n",
    "A counterfactual explanation of a prediction describes the smallest change to the feature values that changes the prediction to a predefined output.\n",
    "\n",
    "What makes a counterfactual explanation a good explanation: \n",
    "\"An obvious first requirement is that a counterfactual instance produces the predefined prediction as closely as possible. [...] counterfactual should be as similar as possible to the instance regarding feature values [...]The counterfactual should not only be close to the original instance, but should also change as few features as possible. [...] Third, it is often desirable to generate multiple diverse counterfactual explanations so that the decision-subject gets access to multiple viable ways of generating a different outcome. [...] The last requirement is that a counterfactual instance should have feature values that are likely. \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0794366",
   "metadata": {},
   "source": [
    "**b)** Consider the model for california housing in task 1 a). Try to find at least 2 counterfactual explanations for the first instance in the test set that would at least double the predicted value. \n",
    "What are the advantages and disadvantages of counterfactual explanations? Use the counterfactual explanations you found to vividly explain the advantages/disadvantages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d87f03be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MedInc           4.151800\n",
       "HouseAge        22.000000\n",
       "AveRooms         5.663073\n",
       "AveBedrms        1.075472\n",
       "Population    1551.000000\n",
       "AveOccup         4.180593\n",
       "Latitude        32.580000\n",
       "Longitude     -117.050000\n",
       "Name: 14740, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "105b1797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.58096968])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test.iloc[0:1]) # predicted value that should be doubled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec59493a",
   "metadata": {},
   "source": [
    "**manipulate input to get a counterfactual prediction, i.e. set new values for certain variables such that the predicted value doubles (trial and error)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8e4c2e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.06671371])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manipulated_instance = X_test.iloc[0:1].replace(to_replace = {'MedInc':X_test['MedInc'].iloc[0]}, value= '2')\n",
    "model.predict(manipulated_instance) # not yet at desired level -> increas MedInc further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bbc85cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.5203337])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manipulated_instance = X_test.iloc[0:1].replace(to_replace = {'MedInc':X_test['MedInc'].iloc[0]}, value= '8')\n",
    "model.predict(manipulated_instance) # prediction now above desired level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c0409fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.60136852])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manipulated_instance = X_test.iloc[0:1].replace(to_replace = {'Longitude':X_test['Longitude'].iloc[0]}, value= '-150')\n",
    "model.predict(manipulated_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a471e4",
   "metadata": {},
   "source": [
    "Advantages listed in 9.3: \"The interpretation of counterfactual explanations is very clear. [...] The counterfactual method does not require access to the data or the model.[...] The method works also with systems that do not use machine learning. [...] The counterfactual explanation method is relatively easy to implement.\"\n",
    "Disadvantages: \"For each instance you will usually find multiple counterfactual explanations (Rashomon effect). \"\n",
    "\n",
    "As we can see from the example above, a sequence of trial and error in replacing certain feature values leads to the desired counterfactual of the predictive value being doubled. However, there are many such combinations possible (Rashomon effect) and some values might be unrealistic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af19b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd088dc94f85831c7bd35a321f0de4bbf881cfe74aa5b6804b5ee4d1f1a4d445235"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
