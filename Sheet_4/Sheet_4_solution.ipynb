{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10758e62",
   "metadata": {},
   "source": [
    "# Interpretable Machine Learning\n",
    "## Exercise Sheet: 4\n",
    "## This exercise sheet covers chapters 6, 8.1 + 8.2 from the IML book by Christoph Molnar\n",
    "\n",
    "Kristin Blesch (blesch@leibniz-bips.de)<br>\n",
    "Niklas Koenen (koenen@leibniz-bips.de)\n",
    "<hr style=\"border:1.5px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1940ab9",
   "metadata": {},
   "source": [
    "# 1) Partial Dependence Plot (PDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f7a3b1",
   "metadata": {},
   "source": [
    "In this task, the concept of Partial Dependence Plots (PDP) will be explained. For this, you have to train a multilayer perceptron (MLP) on the regression dataset `sklearn.datasets.fetch_california_housing` and then implement the PDP method on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4135376",
   "metadata": {},
   "source": [
    "## a) Data\n",
    "Load the data and get familiar with it by extracting the features $X$ as a `pandas.DataFrame` and the target $Y$. Then, answer the following questions:\n",
    "\n",
    "- What are the features and what are their types (numeric, binary, categorical)?\n",
    "- What is the target outcome?\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e1c1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "cal_housing = fetch_california_housing()\n",
    "\n",
    "# Get features as pd.DataFrame\n",
    "X = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\n",
    "\n",
    "# Get target\n",
    "Y = cal_housing.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13f2ab",
   "metadata": {},
   "source": [
    "*   The dataset includes the following features\n",
    "  * `MedInc`: Median income in block group (numerical)\n",
    "  * `HouseAge`: Median house age in block group (numerical)\n",
    "  * `AveRooms`: Average number of rooms per household (numerical)\n",
    "  * `AveBedrms`: Average number of bedrooms per household (numerical)\n",
    "  * `Population`: Block group population (numerical)\n",
    "  * `AveOccup`: Average number of household members (numerical)\n",
    "  * `Latitude`: Block group latitude (numerical)\n",
    "  * `Longitude`: Block group longitude (numerical)\n",
    "\n",
    "*   The target variable is the median house value for California districts in hundreds of thousands of dollars ($100,000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914386d",
   "metadata": {},
   "source": [
    "## b) Train MLP\n",
    "Create training and test data using the function `train_test_split` and train the predefined MLP model. Then calculate the $R^2$ value on the test data using the [score](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor.score) method.\n",
    "\n",
    "**Hint:** Use function `fit(X_train, y_train)`.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1771ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state = 0)\n",
    "\n",
    "# Define model\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "model =  make_pipeline(\n",
    "    QuantileTransformer(),\n",
    "    MLPRegressor(\n",
    "        hidden_layer_sizes=(50, 50), learning_rate_init=0.01, early_stopping=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train the model on the trainings data\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R^2- value\n",
    "\n",
    "f\"Test R2 score: {model.score(X_test, y_test):.2f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf301a",
   "metadata": {},
   "source": [
    "## c) Implement PDP\n",
    "**i)** Now let's apply the method to the feature `HouseAge`. Crate a PDP for this feature.\n",
    "\n",
    "**Hint:** Get the minimum and maximum value of this feature and create a vector with the intermediate values (`np.linspace(min, max, 100)`). Insert these values into the test data in a loop and calculate the average output. Finally, create a plot with `plt.plot`.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the domain for feature 'HouseAge'\n",
    "min_value = min(X_test.HouseAge)\n",
    "max_value = max(X_test.HouseAge)\n",
    "HouseAge = np.linspace(min_value, max_value, 100)\n",
    "\n",
    "# Calculate PDP\n",
    "y = []\n",
    "for i in HouseAge:\n",
    "  x = X_test.copy()\n",
    "  x.HouseAge = i\n",
    "  y.append(np.mean(model.predict(x)))\n",
    "\n",
    "# Plot PDP\n",
    "plt.plot(HouseAge, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1134d724",
   "metadata": {},
   "source": [
    "**ii)** Create another PDP for this feature with the predefined function [`PartialDependenceDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.PartialDependenceDisplay.html#sklearn-inspection-partialdependencedisplay) and check your implementation.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    model,\n",
    "    X_test,\n",
    "    features = [\"HouseAge\"],\n",
    "    grid_resolution=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80186cb",
   "metadata": {},
   "source": [
    "**iii)** What do these plots tell us? Can they indicate the importance of the feature and if not, how do you get it?\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d27b89",
   "metadata": {},
   "source": [
    "A Partial Dependence Plot describes the influence of a feature on the target on average over the marginal distribution, i.e. every realization in the dataset is considered. In addition, it illustrates the relationship between one feature and the outcome, e.g. is it linear or non-linear? A flat PDP indicates that the feature is less important, and the more the PDP varies, the more important is the feature.\n",
    "\n",
    "However, a single PDP can not be used to determine the importance of a feature because the variations need to be considered in relation to the other PDPs. This problem solves the following partial dependence-based feature importance measure proposed by Greenwell: For feature $i \\in \\{1, \\ldots, K\\}$ and $n$ samples\n",
    "$$\n",
    "I_i(x) = \\sqrt{\\frac{1}{K - 1} \\sum_{k=1}^K\\left( \\hat{f}_i(x) - \\frac{1}{K} \\sum_{j = 1}^K \\hat{f}_{j}(x)\\right)^2}\\quad \\text{with}\\quad \\hat{f}_i(x) = \\frac{1}{n} \\sum_{k = 1}^n \\hat{f}(x, x_C^{(k)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5624ef31",
   "metadata": {},
   "source": [
    "# 2) Accumulated Local Effects (ALE) Plots\n",
    "\n",
    "## a) Theory\n",
    "\n",
    "\n",
    "**i)** What is the problem with PDPs and why can't we blindly trust PDP-based Feature Importance?\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293fb3a",
   "metadata": {},
   "source": [
    "The major problem with PDPs is that they only consider the marginal effect and assume that the features are uncorrelated among each other. When the features are correlated, we create in the calculated mean new data points in areas of the feature distribution where the actual probability is very low. In addition, when considering the marginal effect, values may cancel each other out, even though individually they have a very strong impact.\n",
    "\n",
    "All of these points ensure that the PDP-based Feature Importance measure is only useful under sufficient assumptions, and otherwise, it can make misleading statements. In addition, it captures only the main effect of the feature and ignores possible feature interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf6d64f",
   "metadata": {},
   "source": [
    "**ii)** Explain the concepts of PDP, M-Plots and ALE plots and their differences. What are the respective advantages and disadvantages of each method compared to the other two?\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51908e43",
   "metadata": {},
   "source": [
    "**Partial Dependence Plots:** Average the predictions over the marginal feature distribution (marginal expectation)\n",
    "$$\n",
    "\\hat{f}_{i, \\text{PDP}}(x) = \\int_{X_C} \\hat{f}(x, X_C)\\ d\\mathbb{P}(X_C)\n",
    "$$\n",
    "> “Let me show you what the model predicts on average when each data instance has the value v for that feature. I ignore whether the value v makes sense for all data instances.”\n",
    "* Pros: \n",
    "    - intuitive; It is a very naive approach\n",
    "    - easy to implement\n",
    "    - causal interpretation\n",
    "* Cons:\n",
    "    - assumption of independence, i.e. it is assumed that the feature for the PDP is uncorrelated with all other features\n",
    "    - heterogeneous effects might be hidden; it shows the average marginal effect but individual eliminations are not considered\n",
    "    \n",
    "**Marginal-Plots (M-Plots):** Average the predictions over the conditional feature distribution, i.e. we average the predictions conditional on each grid value (in a predefined neighborhood) of the feature of interest, instead of assuming the marginal distribution at each grid value.\n",
    "$$\n",
    "\\hat{f}_{i, \\text{M}}(x) = \\int_{X_C} \\hat{f}(x, X_C)\\ d\\mathbb{P}(X_C \\mid X = x)\n",
    "$$\n",
    "> “Let me show you what the model predicts on average for data instances that have values close to v for that feature. The effect could be due to that feature, but also due to correlated features.”\n",
    "* Pros:\n",
    "    - avoid averaging predictions of unlikely data instances (conditional distribution instead of marginal distribution)\n",
    "* Cons:\n",
    "    - mix the effect of a feature with the effects of all correlated features\n",
    "\n",
    "**Accumulated Local Effect Plots:** Average the changes in the predictions and accumulate them over the grid\n",
    "$$\n",
    "\\hat{f}_{i,ALE}(x) =\n",
    "\\int_{z_{0}}^{x}\\left(\\int_{X_C}\\hat{g}_i(z,X_C)\\ d\\mathbb{P}(X_C|X = z)\\right)dz-\\text{Const}\\quad \\text{with}\\quad \\hat{g}_i(x,x_c)=\\frac{\\partial\\hat{f}_i(x,x_C)}{\\partial x}\n",
    "$$\n",
    ">“Let me show you how the model predictions change in a small”window\" of the feature around v for data instances in that window.\"\n",
    "* Pros:\n",
    "    - unbiased, i.e. they still work when features are correlated\n",
    "    - faster to compute; $\\mathcal{O}(n)$ with $n$ grid points/intervals\n",
    "    - clear interpretation because ALE plots are centered at zero\n",
    "* Cons:\n",
    "    - interpretation of the effect across intervals is not permissible if the features are strongly correlated. The effects are computed per interval (locally) and therefore the interpretation of the effect can only be local.\n",
    "    - can become a bit shaky with a high number of intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878cd846",
   "metadata": {},
   "source": [
    "# b) Example: ALE Plot\n",
    "\n",
    "**i)** Install the package [ALEPython](https://github.com/blent-ai/ALEPython) from Github and create an ALE plot for the example from task **1)** as well for the feature `HouseAge`.\n",
    "\n",
    "**Hint:** See the 'Install' and 'Usage' section on the Github page.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144085c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install package\n",
    "# To use a code chunk as a console the line must start with '!'.\n",
    "\n",
    "!pip install git+https://github.com/MaximeJumelle/ALEPython.git@dev#egg=alepython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ALE plot (set 'monte_carlo = False')\n",
    "\n",
    "from alepython import ale_plot\n",
    "ale_plot(model, X_train, 'HouseAge')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcbe4a7",
   "metadata": {},
   "source": [
    "**II)** How does the ALE plot differ from the PDP in task 1? Give possible reasons for the similarities or differences.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e57a3",
   "metadata": {},
   "source": [
    "* ALE plots show the relative effect by centering at zero\n",
    "* We see that newer houses have a positive effect and older ones have a negative effect on the sales price. We could not see that in the PDP because they are not centered, i.e. no subtraction of the average effect.\n",
    "* In our case, the plots look very similar, which might be due to the very low correlation from this feature to the others.\n",
    "\n",
    "**Note:** Of course, the plots may look different for other seeds (especially at the edges where there are few data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457c848",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
