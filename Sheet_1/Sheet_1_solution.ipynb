{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10758e62",
   "metadata": {},
   "source": [
    "# Interpretable Machine Learning\n",
    "## Exercise Sheet: 1\n",
    "## This exercise sheet covers chapters 1-2 from the IML book by Christoph Molnar\n",
    "\n",
    "Kristin Blesch (blesch@leibniz-bips.de)<br>\n",
    "Niklas Koenen (koenen@leibniz-bips.de)\n",
    "<hr style=\"border:1.5px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd48f8",
   "metadata": {},
   "source": [
    "# 1) Interpretable Machine Learning (IML) basics\n",
    "\n",
    "## a) What is IML about?\n",
    "Describe the meaning of interpretable machine learning in your own words. You might find the definitions given in the book a useful source of inspiration.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb2537b",
   "metadata": {},
   "source": [
    "Your own words should cover aspects of machine learning, i.e. keywords like black-box algorithms, and aspects of general interpretability, i.e. make it understandable for human beings.\n",
    "\n",
    "Some useful definitions:  \n",
    "- \"Interpretability is the degree to which a human can understand the cause of a decision\" given in Miller, Tim. “Explanation in artificial intelligence: Insights from the social sciences.” arXiv Preprint arXiv:1706.07269. (2017)\n",
    "- \"Interpretability is the degree to which a human can consistently predict the model’s result.\" given in Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. “Examples are not enough, learn to criticize! Criticism for interpretability.” Advances in Neural Information Processing Systems (2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9aff3a",
   "metadata": {},
   "source": [
    "## b) Why does interpretablility matter?\n",
    "Give reasons why interpretability matters for constructing a machine learning model and its subsequent application to real world use cases.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98786efd",
   "metadata": {},
   "source": [
    "- learning more about the problem if you know why things are the way they are\n",
    "- debugging and auditing the machine learning model is facilitated\n",
    "- knowing just the prediction result might be insufficient for certain tasks\n",
    "- satisfies curiosity, provide insights why the model came to certain predictions/conclusions enhances acceptance which is especially useful in real-world applications, e.g. business decisions\n",
    "- socially desirable aspects such as fairness, privacy, robustness/consistency, trust can be evaluated for being respected by the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de16ea",
   "metadata": {},
   "source": [
    "# 2) A more granular view on IML\n",
    "\n",
    "## a) Explain the difference between model-specific and model-agnostic IML methods.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56928ccd",
   "metadata": {},
   "source": [
    "- model-specific IML methods: Some prediction algorithms provide the user with quantities that allow the user to interpret the model and hence inherently provide IML methods. Famous examples are regression coefficients in a linear regression model or the variable importance measures based on splits for tree-based models. However, these IML methods are then tied to the predicition algorithm and do not generalize to other predictive algorithms.\n",
    "- model-agnostic IML methids: Model-agnostic IML methods provide interpretability for any given predictive algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef9619d",
   "metadata": {},
   "source": [
    "## b) Contrast global and local model interpretability methods: What kind of explanations do they aim for? \n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebcc556",
   "metadata": {},
   "source": [
    "- global interpretability methods: Explanations regarding the model as a whole, i.e. how the model makes decisions, insights into the model sturture and its parameters, learning about the distribution of the target variable, determine important features and how they interact\n",
    "- local interpretability methods: Explanations for individual predictions, i.e. why a certain prediction for a specfic instance was made"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03750e52",
   "metadata": {},
   "source": [
    "# 3) The human factor\n",
    "Describe how good IML explanations should be designed such that they are easy to grasp for human beings. \n",
    "\n",
    "Bonus discussion: Do you - as a human being yourself - agree with the aspects listed in the book?\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aae77b9",
   "metadata": {},
   "source": [
    "Explanations should be contrastive, select on few relevant causes only, respect the social context, focus on the abnormal, be truthful, be consistent with prior beliefs of the explainee, be general and probable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
