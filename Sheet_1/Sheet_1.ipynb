{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10758e62",
   "metadata": {},
   "source": [
    "# Interpretable Machine Learning\n",
    "## Exercise Sheet: 1\n",
    "## This exercise sheet covers chapters 2-3 from the IML book by Christoph Molnar\n",
    "\n",
    "Kristin Blesch (blesch@leibniz-bips.de)<br>\n",
    "Niklas Koenen (koenen@leibniz-bips.de)\n",
    "<hr style=\"border:1.5px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd48f8",
   "metadata": {},
   "source": [
    "# 1) Interpretable Machine Learning (IML) basics\n",
    "\n",
    "## a) What is IML about?\n",
    "Describe the meaning of interpretable machine learning in your own words. You might find the definitions given in the book a useful source of inspiration.\n",
    "\n",
    "** Interpretable Machine Learning is the subject which focuses on drawing  insights from a supervised machine learning models.\
IML seeks for reasons that causes the model to behave in a certain manner. This is because regardless of prediction accuracy more insights\
concerning the model's results is necessary to guide decision makers. Several techniqes may be blended together to describes the concept of \
intepretability of a ML model. These techniques operates at both global and local levels in general.the general and local overview concerning \
Examples of techniques employed include training models that are directly interpretable such as sparsed linear models and decision trees or\
applying model diagnostic tools such as evaluation of variable importance.
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb2537b",
   "metadata": {},
   "source": [
    "<span style=\"color:#AAAEBC\"> - your solution here - </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9aff3a",
   "metadata": {},
   "source": [
    "## b) Why does interpretablility matter?\n",
    "Give reasons why interpretability matters for constructing a machine learning model and its subsequent application to real world use cases.\n",
    "\n",
    "**  Even though machine learning models can performs very well in terms of accuracy,in practice we need the model to be interpretable\
so we can draws more insights about the problem under study and proves the legitimacy of the predictions. In production, decision makers are \
more interested on why the model gives a particular outcome. For example, in clinical trial we may not only interested on predicting \
treatment effectiveness but also we may be more interested on why the treatment did not work on a particular patient :**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98786efd",
   "metadata": {},
   "source": [
    "<span style=\"color:#AAAEBC\"> - your solution here - </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de16ea",
   "metadata": {},
   "source": [
    "# 2) A more granular view on IML\n",
    "\n",
    "## a) Explain the difference between model-specific and model-agnostic IML methods.\n",
    "\n",
    "** Model specific IML method involves direct training an interpretable machine learning models such as linear models and decision trees.\
Model specific IML technique may be very limited in practice because we always enconter complexities in real life scinarios.\
Model agnostic methods are techniques deviced to aid interpretation of a more complex supervised machine learning models. Example of model\
agnostic tool is the ploting of variable importance and partial dependence plot. Model agnostic IML methods can also be applied to all forms\
of ML models:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56928ccd",
   "metadata": {},
   "source": [
    "<span style=\"color:#AAAEBC\"> - your solution here - </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef9619d",
   "metadata": {},
   "source": [
    "## b) Contrast global and local model interpretability methods: What kind of explanations do they aim for? \n",
    "\n",
    "**Global model interpretability requires knowledge on algorith, the data applied and the trained model in order to \
draw insights about the overall performance of the ML model. Although  it is difficult to achieve in practice, at the global level \
we can understand variables contribution to the general performance of the model and the distribution of the predicted outcomes with \
respect to the features space.\
Local model interpretability focuses on the variation of a single feature with respect to the target. The idea is to understand the model's behaviour\
when a particular covariate is changed. At this level, the complexity due to variables interdependence is nearly eliminated and the feature's target \
relationship is approximately linear which makes interpretation easy and meaningful. Local models interpretabilty also covers a case when we are interested\
on understanding the behaviour of the model when acted on a particular subset of dataset. In this case, severals subjects may selected and the global \
interpretability procedure can applied by treating the subjects as a complete dataset.:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebcc556",
   "metadata": {},
   "source": [
    "<span style=\"color:#AAAEBC\"> - your solution here - </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03750e52",
   "metadata": {},
   "source": [
    "# 3) The human factor\n",
    "Describe how good IML explanations should be designed such that they are easy to grasp for human beings. \n",
    "\n",
    "Bonus discussion: Do you - as a human being yourself - agree with the aspects listed in the book?\n",
    "\n",
    "** Interpretable Machine Learning (IML) explanations must possess several criteria to be easily digestible by human.\
The explanations should be contrastive: The ability to allow parallel comparison with compliment of the model's prediction)\
The explanations need to be selective: Its should be brief and informative i.e, contain few selected variables that can \
explain why the model gives a certain prediction\
Explanation need to be truthful: Meaning that it should predict an event as truthfully as possible i.e the prediction must be\
the same for all instances with the same set of characteristics\
Explanations need to be consistent with prior belief: Althogh it is difficult to comprehend and may badly affect the predictions in practice,\
an explanation that incorporate prior belief concerning the problem is prefered\
The explanations need to be social in nature: Technicalities involved in deliverence must regard the nature of audience/end user of the model\
The explanations must focus on abnormality: In the situation where the abnormal feature influences the target, a good explanation need to prioritize\
the abnormal cause over the normal causes even if they both equally influencing the target.\

Personally as a human, I do agree with most of the aspects that describes good IML explanation but the aspect of including prior belief\
should accompany with more data with characteristics that support the argument. I also think that an explanation should be selective but\
must base on intensive evaluation to isolate the mostly influential features.

:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aae77b9",
   "metadata": {},
   "source": [
    "<span style=\"color:#AAAEBC\"> - your solution here - </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
