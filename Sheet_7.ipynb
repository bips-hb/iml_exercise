{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10758e62",
   "metadata": {},
   "source": [
    "# Interpretable Machine Learning\n",
    "## Exercise Sheet: 7\n",
    "## This exercise sheet covers chapters 9.2 and 9.3 from the IML book by Christoph Molnar\n",
    "Kristin Blesch (blesch@leibniz-bips.de)<br>\n",
    "Niklas Koenen (koenen@leibniz-bips.de)\n",
    "<hr style=\"border:1.5px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd48f8",
   "metadata": {},
   "source": [
    "# 1) LIME \n",
    "\n",
    "**a) Please describe the general idea behind LIME and how the procedure works. Make sure to explain why this method is considered local and what a surrogate model is.**\n",
    "\n",
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c815cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fae288f5",
   "metadata": {},
   "source": [
    "**b) Lime with tabular data**\n",
    "\n",
    "For an application of LIME, consider the gradient boosted california housing model you fitted last week (run code below). \n",
    "\n",
    "Use LIME to give local explanations of the first and the fifth instance of the test data! Use a local ridge regression (default), discretize the data and then give an explanation using 4 features only. Visualize your results using an adequate plot and interpret the output! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbfbe8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76154181, 1.62701009, 2.59560924, ..., 2.73226183, 4.32333119,\n",
       "       0.59749722])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get data\n",
    "cal_housing = fetch_california_housing()\n",
    "X = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\n",
    "y = cal_housing.target\n",
    "\n",
    "# Get train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "# Train model and calculate RÂ²-score\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "model = HistGradientBoostingRegressor(random_state = 42)\n",
    "model.fit(X_train, y_train)\n",
    "model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79321c2",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76cb6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explaination using LIME\n",
    "import numpy as np\n",
    "import lime.lime_tabular\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed6c18",
   "metadata": {},
   "source": [
    "# 2) Counterfactual Explanations\n",
    "\n",
    "\n",
    "**a) What is a counterfactual explanation? Explain the concept using an example. Furthermorre, describe how this perspective applies to the context of interpretable machine learning: Do we explain the causal structure of the data?What makes a counterfactual explanation a good explanation?**\n",
    "\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0794366",
   "metadata": {},
   "source": [
    "**b) Consider the model for california housing in task 1 a). Try to find at least 2 counterfactual explanations for the first instance in the test set that would at least double the predicted value (Hint: You may use trial and error).\n",
    "What are the advantages and disadvantages of counterfactual explanations? Use the counterfactual explanations you found to vividly explain the advantages/disadvantages!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd9a2fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MedInc           4.151800\n",
       "HouseAge        22.000000\n",
       "AveRooms         5.663073\n",
       "AveBedrms        1.075472\n",
       "Population    1551.000000\n",
       "AveOccup         4.180593\n",
       "Latitude        32.580000\n",
       "Longitude     -117.050000\n",
       "Name: 14740, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cf2d480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.58096968])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test.iloc[0:1]) # predicted value that should be doubled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb8852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd088dc94f85831c7bd35a321f0de4bbf881cfe74aa5b6804b5ee4d1f1a4d445235"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
