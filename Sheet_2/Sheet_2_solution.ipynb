{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# Interpretable Machine Learning\n",
    "## Exercise Sheet: 2\n",
    "\n",
    "### This exercise sheet covers chapters 5.1+5.2 from the IML book by Christoph Molnar\n",
    "Kristin Blesch (blesch@leibniz-bips.de)<br>\n",
    "Niklas Koenen (koenen@leibniz-bips.de)\n",
    "<hr style=\"border:1.5px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# 1) Linear Regression\n",
    "The linear model is one of the first and best-studied models for regression\n",
    "problems. However, it makes some (strong) assumptions about the given data. \n",
    "In the following exercise, you will learn how to fit and interpret a linear \n",
    "model and investigate its limitations. \n",
    "\n",
    "Let's consider a linear regression model \n",
    "for three features $p=3$ of the form\n",
    "$$\n",
    "\\hat{f}(x) = \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + x_3\\beta_3 = X \\beta\n",
    "$$\n",
    "and the data is given in the csv-file `sheet_2_lin_data.csv`.\n",
    "\n",
    "**a)** Load the data as a `pandas` data frame and visualize in a 2D plot how each \n",
    "feature ($X_1, X_2, X_3$) affects the output $Y$. Preprocess the loaded data frame\n",
    "with `pandas.melt` and use the function `seaborn.scatterplot` with specified \n",
    "grouping argument (`hue`) for the visualization.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set_theme() # Because seaborn theme looks awesome\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data from csv-file as a pandas data frame\n",
    "linear_data = pd.read_csv(\"sheet_2_lin_data.csv\")\n",
    "# Preprocess the data frame so that we have only the columns \"Y\", \"X\" and \"feature\"\n",
    "dat = pd.melt(linear_data, \n",
    "              id_vars = ['Y'], \n",
    "              value_vars = [\"X1\", \"X2\", \"X3\"],\n",
    "              var_name = \"feature\",\n",
    "              value_name = \"X\")\n",
    "# Plot the data\n",
    "plt.figure(figsize=(15, 10))\n",
    "fig = sb.scatterplot(data = dat, x = \"X\", y = \"Y\", hue = \"feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**b)** Determine the _design matrix_ $X$ of the given data as a `numpy` array (don't \n",
    "forget the column of ones for the intercept $\\beta_0$). Using this matrix $X$, \n",
    "estimate the regression coefficients $\\beta$ of the linear model by the method of least \n",
    "squares and calculate the corresponding standard error of the estimates.  \n",
    "**Hint:** The estimated linear regression coefficients are given by \n",
    "$\\hat{\\beta} = (X^T X)^{-1} X^T Y$ and the standard error\n",
    "is given by the square root of the diagonal entries of $(X^TX)^{-1} \\hat{\\sigma}^2$ with \n",
    "$\\hat{\\sigma}^2 = \\frac{1}{n - p} \\sum_{i=1}^n (y_i- \\hat{y}_i)^2$.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Determine the design matrix X\n",
    "X = linear_data.copy()\n",
    "X.insert(1, \"X0\", 1)\n",
    "X = X.loc[:,\"X0\":\"X3\"].to_numpy()\n",
    "\n",
    "# Calculate the estimated regression coefficients (X * X^T)^-1\n",
    "X_XT_inv = np.linalg.inv(np.matmul(X.T, X))\n",
    "\n",
    "# Outcome Y as a numpy array\n",
    "Y = linear_data[\"Y\"].to_numpy()\n",
    "\n",
    "# beta_hat = (X * X^T)^-1 * X^T * Y (see hint)\n",
    "beta_hat = np.matmul(X_XT_inv, np.matmul(X.T, Y))\n",
    "\n",
    "# Calculate the standard error estimate variance (see hint)\n",
    "sigma_2 = np.sum((np.matmul(X, beta_hat) - Y)**2) / (X.shape[0] - 3)\n",
    "standard_error = np.diag(sigma_2 * X_XT_inv)**0.5\n",
    "\n",
    "# Print the result\n",
    "pd.DataFrame(data = {\"beta_hat\": beta_hat, \"SE\": standard_error}, index = [\"b0\", \"b1\", \"b2\", \"b3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**c)** Calculate the $R^2$-value. What does the $R^2$-value mean in this context?\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the squared sum of the error terms\n",
    "sse = np.sum((np.matmul(X, beta_hat) - Y)**2)\n",
    "\n",
    "# Calculate the squared sum of the data variance\n",
    "sst = np.sum((Y - np.mean(Y))**2)\n",
    "\n",
    "# Calculate and print the R²-value\n",
    "\"R²-value: {:.4f}\".format(1 - sse / sst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R²-value takes values between 0 and 1 and describes how much variance in the true outcome is explained by the model and how well the model describes the given data. Since in this case the R²-value is almost 1, it is a very good fit and if we look only at the prediction performance, the linear model is a perfect choice for the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**d)** What happens to the outcome if we increase the first feature value $X_1$ by three? Which feature\n",
    "is the most important one?\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The outcome Y increases by\n",
    "beta_hat[1] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature importance\n",
    "importance = np.abs(beta_hat / standard_error)\n",
    "print(importance)\n",
    "\n",
    "# Get the most important feature\n",
    "most_important_feature = np.argmax(importance)\n",
    "\n",
    "# Print the most important feature\n",
    "\"X\" + str(most_important_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**e)** The data was sampled by the data generating process \n",
    "$$f(x) = x_1 + 2 x_2 + 3 x_3 + \\varepsilon.$$ \n",
    "Does the most important feature from part d) make sense regarding the data \n",
    "generating process? Explain your answer!\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a first look: No! If you look at the data generating process, $x_3$ would have to be the most important feature because it gets the highest weighting with $3$ i.e. it has the biggest influence on output $y$. Apart from the intercept, $x_1$ would have to be the least important feature because it has the smallest weight. Thus, it seems that not all the requirements for a linear model are satisfied.\n",
    "\n",
    "For example, the input features $x_1$ and $x_3$ are highly correlated, which screws up the estimation of the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation\n",
    "sb.heatmap(linear_data.corr(), annot = True, cmap = \"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<hr style=\"border:1.5px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# 2) Logistic Regression\n",
    "\n",
    "## a) Introduction of odds ratio\n",
    "**i)** Suppose we have an ordinary and uniformly distributed six-sided\n",
    "dice described by a random variable $Y_1$. What are the odds of rolling a \n",
    "$5$ or a $6$?\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds is defined by the probability of an event divided by\n",
    "the probability of its counter event, i.e. $\\text{odds} = \\frac{\\mathbb{P}(Y_1 \\geq 5)}{\\mathbb{P}(Y_1 < 5)}$. Hence\n",
    "$$\n",
    "\\text{odds}_{Y_1} = \\frac{\\mathbb{P}(Y_1 \\geq 5)}{\\mathbb{P}(Y_1 < 5)} = \\frac{\\mathbb{P}(Y_1 \\geq 5)}{1 - \\mathbb{P}(Y_1 \\geq 5)} = \\frac{\\frac{2}{6}}{1 - \\frac{2}{6}} = \\frac{1}{2} \\hat{=}\\ \\text{\"1-to-2\"}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**ii)** Let's consider a loaded dice $Y_2$ that rolls a $6$ with a probability of $\\frac{1}{3}$\n",
    "and a $5$ with a probability of $\\frac{1}{12}$. What are the odds for the event \n",
    "$Y_2 \\geq 5$ now?\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get\n",
    "$$\n",
    "\\text{odds}_{Y_2} = \\frac{\\mathbb{P}(Y_2 \\geq 5)}{\\mathbb{P}(Y_2 < 5)} = \\frac{\\frac{1}{3} + \\frac{1}{12}}{1 - \\left(\\frac{1}{3} + \\frac{1}{12}\\right)} = \\frac{\\frac{5}{12}}{1 - \\frac{5}{12}} = \\frac{5}{7}\\hat{=}\\ \\text{\"5-to-7\"}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**iii)** How have the odds changed from dice $Y_1$ to $Y_2$, i.e. by how much is it\n",
    "higher/lower? This factor is called the _odds ratio_.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{odds ratio} = \\frac{\\text{odds}_{Y_2}}{\\text{odds}_{Y_1}} = \\frac{\\frac{5}{7}}{\\frac{1}{2}} = \\frac{10}{7} \\approx 1.4286.\n",
    "$$\n",
    "This means that the odds of rolling a number greater than $5$ have increased by\n",
    "$1.4285$ times if the second dice is thrown instead of the first one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## b) Logistic Regression in Python\n",
    "In this exercise, a logistic model is fitted on the [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) indicating whether \n",
    "a given plant of the iris genus is a member of the species 'virginica' or not. For \n",
    "this purpose, we use the mighty package [`scikit-learn`](https://scikit-learn.org/stable/).\n",
    "\n",
    "**i)** Load the iris dataset from `sklearn.datasets` and save the features with \n",
    "corresponding output in one `pandas` dataframe. Then adjust the output to the \n",
    "binary classification problem described above.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load the iris dataset\n",
    "iris_dataset = load_iris()\n",
    "\n",
    "# I changed the feature names a bit (not necessary)\n",
    "feature_names = [i.rsplit(' ', maxsplit = 1)[0] for i in iris_dataset.feature_names]\n",
    "\n",
    "# Move the iris dataset to pandas\n",
    "iris_df = pd.DataFrame(iris_dataset.data, columns = feature_names)\n",
    "\n",
    "# Adjust the output, remember  0 = 'setosa', 1 = 'versicolour' and 2 = 'virginica'\n",
    "iris_df['virginica'] = iris_dataset.target == 2\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**ii)** Visualize this dataframe with `seaborn.pairplot` with grouping argument `virginica`.\n",
    "What do you think which feature incremented by one unit increases the odds for the class `virginica` the most? Explain your choice.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess that increasing features $3$ or $4$ will increase the chances for the class 'virgincia' the most. Because:\n",
    "\n",
    "1. Distribution of one feature: If we look at the images on the diagonal, we notice that the distributions of the individual input features are different. But in the case of the first two, large overlaps are already visible and a shift of the blue (not 'virgincia') by one to the right (corresponding to an increase of the input feature by 1) would further increase them. But if input features 3 or 4 are increased by one each, there are much more new overlaps, i.e. the difference from before to after is much larger than for the first two features.\n",
    "\n",
    "2. Distribution of a pair of features: Clearly, with an increase of 1, we want the considered data point to be moved into the cluster of data points for the 'virginica' class. For example, if we increase the feature 'sepal width', we move away from both distributions, i.e. it would not increase our chances for the class 'virginica'. The most overlap would be for features 3 and 4, which is why those are my guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the dataframe\n",
    "fig = sb.pairplot(data = iris_df, hue = 'virginica', height = 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**iii)** Fit a regression model with the function `sklearn.linear_model.LogisticRegression` and extract the estimated model parameters $\\beta$ from it. Now we want to determine \n",
    "the odds ratio of the parameters. Compared to the simple example from part a), \n",
    "a logistic model provides a probability conditional on the input data $x$, i.e. \n",
    "the odds is given by\n",
    "$$\n",
    "\\text{odds}_\\text{logreg} = \\frac{\\mathbb{P}(Y = 1 \\mid x)}{1- \\mathbb{P}(Y = 1 \\mid x)}.\n",
    "$$\n",
    "Therefore instead of using another dice, the stochastic model is now modified by increasing a component of $x$. Denote with $x_{x_i \\to x_i +1}$ a vector where the $i$-th component (feature) is increased by one unit. Thus, the following representation of the odds ratio for the i-th parameter results from the lecture:\n",
    "$$\n",
    "\\text{odds ratio}_i = \\frac{\\frac{\\mathbb{P}(Y = 1 \\mid x_{x_i \\to x_i +1})}{1- \\mathbb{P}(Y = 1 \\mid x_{x_i \\to x_i +1})}}{\\frac{\\mathbb{P}(Y = 1 \\mid x)}{1- \\mathbb{P}(Y = 1 \\mid x)}} = \\exp(\\beta_i).\n",
    "$$\n",
    "What are the odds ratios of your fitted logistic regression model, and which \n",
    "feature increases the odds for the class 'virginica' the most? How does your \n",
    "choice fit with your thoughts from part II)?\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Get the input and outcome of your iris dataframe and move it to numpy\n",
    "X = iris_df.iloc[:, 0:-1].to_numpy()\n",
    "Y = iris_df.iloc[:, -1].to_numpy()\n",
    "\n",
    "# Fit a logistic regression model on these data\n",
    "model_logreg = LogisticRegression().fit(X, Y)\n",
    "\n",
    "# Plot the result in a clear table\n",
    "beta = np.append(model_logreg.intercept_, model_logreg.coef_)\n",
    "pd.DataFrame(data = {\"label\": [\"b0\", \"b1\", \"b2\", \"b3\", \"b4\"], \"beta\": beta, \"Odds ratio\": np.round(np.exp(beta), 2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    " # 3) Prediction of Purchase Prices of Houses\n",
    "Load the dataset `sheet_2_house_price_data.csv`. This dataset contains the purchase prices of $10.000$ houses in Germany and includes the `size` (in m²), `location` ($0=$ bad, $1=$ good, $2=$ very good and $3=$ luxury) and year of construction (`year`).\n",
    "\n",
    "**a)** Fit a linear regression model and calculate the mean squared error (MSE) of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Prepare dataset for training\n",
    "house_data = pd.read_csv(\"sheet_2_house_price_data.csv\")\n",
    "X = house_data.iloc[:, 0:-1].to_numpy()\n",
    "Y = house_data.iloc[:, -1].to_numpy()\n",
    "\n",
    "# Fit a linear regression model\n",
    "linear_model = LinearRegression().fit(X,Y)\n",
    "\n",
    "# Calculate the MSE\n",
    "'MSE:  ' + str(np.round(np.mean((linear_model.predict(X) - Y)**2), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** How can this dataset be examined to see whether feature interactions affect the output and, if so, which one? A visual explanation is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test for interactions 'size' ~ 'location'\n",
    "fig = sb.FacetGrid(house_data, col=\"location\", height = 6)\n",
    "fig.map_dataframe(sb.scatterplot, x=\"size\", y=\"price\")\n",
    "fig.add_legend()\n",
    "fig.set_xlabels(\"size\")\n",
    "fig = fig.set_ylabels(\"price\")\n",
    "\n",
    "# Seems to be interacting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming they would not interact, a change in 'location' should only provide a uniform shift with respect to the y-axis, since this would be a constant in the linear model. But the slope changes when the location is changed, so it must be an interaction of 'size' ~ ' location'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test for interactions 'size' ~ 'year'\n",
    "plt.figure(figsize=(15, 10))\n",
    "fig = sb.scatterplot(data = house_data, x=\"size\", y=\"price\", hue = \"year\")\n",
    "\n",
    "# Seems to be interacting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming they would not interact, a change in 'year' should only provide a uniform shift with respect to the y-axis, since this would be a constant in the linear model. But the slope changes when the year is changed, so it must be an interaction of 'size' ~ ' year'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test for interactions 'year' ~ 'location'\n",
    "fig = sb.FacetGrid(house_data, col=\"location\", height = 6)\n",
    "fig.map_dataframe(sb.scatterplot, x=\"year\", y=\"price\")\n",
    "fig.add_legend()\n",
    "fig.set_xlabels(\"year\")\n",
    "fig = fig.set_ylabels(\"price\")\n",
    "\n",
    "# Seems to be non-interacting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All plots look the same (except for a shift in height), so no interaction (at least not a strong one) is apparent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Fit a linear regression model on these data by manually adding the relevant interactions. Then calculate the MSE again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data_interact = house_data\n",
    "house_data_interact['size_year'] = house_data_interact['size'] * house_data_interact['year']\n",
    "house_data_interact['size_loc'] = house_data_interact['size'] * house_data_interact['location']\n",
    "\n",
    "X = house_data_interact.iloc[:, [0,1,2,4,5]].to_numpy()\n",
    "Y = house_data_interact.iloc[:, [3]].to_numpy()\n",
    "\n",
    "# Fit a linear regression model\n",
    "linear_model = LinearRegression().fit(X,Y)\n",
    "\n",
    "# Calculate the MSE\n",
    "'MSE:  ' + str(np.round(np.mean((linear_model.predict(X) - Y)**2), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Advanced: Logistic Regression\n",
    "Similar to linear regression, logistic regression assumes that there are no interactions between the features, even though this is rarely the case in reality. This also means that the odds ratio is constant for the estimated parameters $\\beta$ independent of the input data $x$, although the logistic model outputs a probability conditional on the data ($\\mathbb{P}(Y = 1 \\mid x)$).\n",
    "Let's consider the following logistic model with an interaction term and numerical features\n",
    "$$\n",
    "\\mathbb{P}(Y = 1 \\mid x) = f(x) = \\text{logistic}\\left(\\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + x_3 \\beta_3 + \\beta_{12} x_1 x_2 + \\beta_{33} x_3^2\\right).\n",
    "$$\n",
    "\n",
    "Calculate the odds ratio for each parameter $\\beta_1, \\beta_2$ and $\\beta_3$ if we\n",
    "increase the respective feature value by one unit.\n",
    "\n",
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It holds for $x$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{odds} &= \\frac{\\mathbb{P}(Y = 1 \\mid x)}{ 1 - \\mathbb{P}(Y=1 \\mid x)} = \\frac{f(x)}{1-f(x)} = \\frac{\\frac{1 }{1 + \\exp\\left(- \\left( \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + x_3 \\beta_3 + \\beta_{12} x_1 x_2 + \\beta_{33} x_3^2\\right) \\right)}}{1 - \\frac{1 }{1 + \\exp\\left(- \\left( \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + x_3 \\beta_3 + \\beta_{12} x_1 x_2 + \\beta_{33} x_3^2\\right) \\right)}} \\\\\n",
    "& = \\frac{1 }{1 + \\exp\\left(- \\left( \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + x_3 \\beta_3 + \\beta_{12} x_1 x_2 + \\beta_{33} x_3^2\\right) \\right) -1} \\\\\n",
    "&= \\exp\\left( \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + x_3 \\beta_3 + \\beta_{12} x_1 x_2 + \\beta_{33} x_3^2\\right) \n",
    "\\end{align}.\n",
    "$$\n",
    "\n",
    "Hence, we get\n",
    "$$\n",
    "\\text{odds ratio}_{X_1} = \\frac{\\exp\\left( \\beta_0 + (x_1 +1) \\beta_1 + x_2 \\beta_2 + x_3 \\beta_3 + \\beta_{12} (x_1 + 1) x_2 + \\beta_{33} x_3^2\\right)}{\\exp\\left( \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + x_3 \\beta_3 + \\beta_{12} x_1 x_2 + \\beta_{33} x_3^2\\right)} = \\exp\\left(\\beta_1 + \\beta_{12} x_2\\right)\n",
    "$$\n",
    "$$\n",
    "\\text{odds ratio}_{X_2} = \\frac{\\exp\\left( \\beta_0 + x_1 \\beta_1 + (x_2 + 1) \\beta_2 + x_3 \\beta_3 + \\beta_{12} x_1 (x_2 + 1) + \\beta_{33} x_3^2\\right)}{\\exp\\left( \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + x_3 \\beta_3 + \\beta_{12} x_1 x_2 + \\beta_{33} x_3^2\\right)} = \\exp\\left(\\beta_2 + \\beta_{12} x_1\\right)\n",
    "$$\n",
    "$$\n",
    "\\text{odds ratio}_{X_3} = \\frac{\\exp\\left( \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + (x_3 + 1) \\beta_3 + \\beta_{12} x_1 x_2 + \\beta_{33} (x_3 +1)^2\\right)}{\\exp\\left( \\beta_0 + x_1 \\beta_1 + x_2 \\beta_2 + x_3 \\beta_3 + \\beta_{12} x_1 x_2 + \\beta_{33} x_3^2\\right)} = \\exp\\left(2\\beta_3 + 2\\beta_{33} x_3\\right)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
